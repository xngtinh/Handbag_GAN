{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DhogRGwxhRPo"
   },
   "outputs": [],
   "source": [
    "!cp '/content/drive/MyDrive/Colab Notebooks/tote256_17k_256_10k.zip' '/content/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g0RqdflxV0MN",
    "outputId": "6483ebdd-8328-45bd-cb03-239546a094b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eiAa0PLiigPm"
   },
   "outputs": [],
   "source": [
    "!unzip '/content/tote256_17k_256_10k.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tGZm2sUJkoZO"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "\n",
    "from keras.models import Sequential, load_model,Model\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5BtLZ4S0aYxS"
   },
   "outputs": [],
   "source": [
    "pip install torch===1.4.0 torchvision===0.5.0 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xo_fi0u9kpjO"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ConvTranspose2d:Applies a 2D transposed convolution operator over an input image composed of several input planes;\n",
    "\n",
    "\"\"\"\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, nz=128, channels=3):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        self.nz = nz\n",
    "        self.channels = channels\n",
    "        \n",
    "        def convlayer(n_input, n_output, k_size=4, stride=2, padding=0):\n",
    "            block = [\n",
    "                nn.ConvTranspose2d(n_input, n_output, kernel_size=k_size, stride=stride, padding=padding, bias=False),\n",
    "                nn.BatchNorm2d(n_output),\n",
    "                nn.ReLU(inplace=True),\n",
    "            ]\n",
    "            return block\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *convlayer(self.nz, 1024, 4, 1, 0), # Fully connected layer via convolution.\n",
    "            *convlayer(1024, 512, 4, 2, 1),\n",
    "            *convlayer(512, 256, 4, 2, 1),\n",
    "            *convlayer(256, 128, 4, 2, 1),\n",
    "            *convlayer(128, 64, 4, 2, 1),\n",
    "            *convlayer(64, 32, 4, 2, 1),\n",
    "            nn.ConvTranspose2d(32, self.channels, 4, 2, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        z = z.view(-1, self.nz, 1, 1)\n",
    "        img = self.model(z)\n",
    "        return img\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, channels=3):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        self.channels = channels\n",
    "\n",
    "        def convlayer(n_input, n_output, k_size=4, stride=2, padding=0, bn=False):\n",
    "            block = [nn.Conv2d(n_input, n_output, kernel_size=k_size, stride=stride, padding=padding, bias=False)]\n",
    "            if bn:\n",
    "                block.append(nn.BatchNorm2d(n_output))\n",
    "            block.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return block\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *convlayer(self.channels, 32, 4, 2, 1),\n",
    "            *convlayer(32, 64, 4, 2, 1),\n",
    "            *convlayer(64, 128, 4, 2, 1, bn=True),\n",
    "            *convlayer(128, 256, 4, 2, 1, bn=True),\n",
    "            *convlayer(256, 512, 4, 2, 1, bn=True),\n",
    "            *convlayer(512, 1024, 4, 2, 1, bn=True),\n",
    "            nn.Conv2d(1024, 1, 4, 1, 0, bias=False),  # FC with Conv.  \n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "                \n",
    "    def forward(self, imgs):\n",
    "        out = self.model(imgs)\n",
    "        return out.view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AlKPVFFOkuoe"
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "lr = 0.001\n",
    "beta1 = 0.5\n",
    "epochs = 30\n",
    "\n",
    "real_label = 0.5\n",
    "fake_label = 0\n",
    "nz = 128\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SKxs11A9P6F9"
   },
   "outputs": [],
   "source": [
    "ROOT = \"/content/drive/MyDrive/Colab Notebooks/model_cGan/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fs1JSoUjkwZW"
   },
   "outputs": [],
   "source": [
    "class DogDataset(Dataset):\n",
    "    def __init__(self, img_dir, transform1=None, transform2=None):\n",
    "    \n",
    "        self.img_dir = img_dir\n",
    "        self.img_names = os.listdir(img_dir)\n",
    "        self.transform1 = transform1\n",
    "        self.transform2 = transform2\n",
    "        \n",
    "        self.imgs = []\n",
    "        for img_name in self.img_names:\n",
    "            img = Image.open(os.path.join(img_dir, img_name))\n",
    "            \n",
    "            if self.transform1 is not None:\n",
    "                img = self.transform1(img)\n",
    "                \n",
    "            self.imgs.append(img)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = self.imgs[index]\n",
    "        \n",
    "        if self.transform2 is not None:\n",
    "            img = self.transform2(img)\n",
    "        \n",
    "        return img\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2u5S1p-vkzNW"
   },
   "outputs": [],
   "source": [
    "transform1 = transforms.Compose([transforms.Resize((256,256))])\n",
    "\n",
    "# Data augmentation and converting to tensors\n",
    "random_transforms = [transforms.RandomRotation(degrees=5)]\n",
    "transform2 = transforms.Compose([transforms.RandomHorizontalFlip(p=0.5),\n",
    "                                 transforms.RandomApply(random_transforms, p=0.3), \n",
    "                                 transforms.ToTensor(),\n",
    "                                 transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "                                 \n",
    "train_dataset = DogDataset(img_dir='/content/tote256_17k_256_10k',\n",
    "                           transform1=transform1,\n",
    "                           transform2=transform2)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=4)\n",
    "\n",
    "                                           \n",
    "imgs = next(iter(train_loader))\n",
    "imgs = imgs.numpy().transpose(0, 2, 3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nBvVJi88lVpv"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(25, 16))\n",
    "for ii, img in enumerate(imgs):\n",
    "    ax = fig.add_subplot(4, 8, ii + 1, xticks=[], yticks=[])\n",
    "    \n",
    "    plt.imshow((img+1)/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DjLfO0YzFhTt"
   },
   "outputs": [],
   "source": [
    "import tensorflow.keras\n",
    "from keras.utils import plot_model\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k64LMgczlZr2"
   },
   "outputs": [],
   "source": [
    "netG = Generator(nz).to(device)\n",
    "netD = Discriminator().to(device)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "\n",
    "\n",
    "fixed_noise = torch.randn(25, nz, 1, 1, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sQjatnBce_NP"
   },
   "outputs": [],
   "source": [
    "def show_generated_img():\n",
    "    noise = torch.randn(1, nz, 1, 1, device=device)\n",
    "    gen_image = netG(noise).to(\"cpu\").clone().detach().squeeze(0)\n",
    "    gen_image = gen_image.numpy().transpose(1, 2, 0)\n",
    "    plt.imshow((gen_image+1)/2)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b--3mDurlmEO"
   },
   "outputs": [],
   "source": [
    "def train(batch_size = 32, epochs = 30, real_label = 0.5, nz = 128):\n",
    "  for epoch in range(epochs):\n",
    "    \n",
    "      for ii, real_images in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
    "          ############################\n",
    "          # (1) Update D network\n",
    "          ###########################\n",
    "          netD.zero_grad()\n",
    "          real_images = real_images.to(device)\n",
    "          batch_size = real_images.size(0)\n",
    "          labels = torch.full((batch_size, 1), real_label, device=device)\n",
    "          outputR = netD(real_images)\n",
    "          noise = torch.randn(batch_size, nz, 1, 1, device=device)\n",
    "          fake = netG(noise)\n",
    "          outputF = netD(fake.detach())\n",
    "          errD = (torch.mean((outputR - torch.mean(outputF) - labels) ** 2 ) + \n",
    "                  torch.mean((outputF - torch.mean(outputR) + labels) ** 2 ))/2\n",
    "          errD.backward(retain_graph=True)\n",
    "          optimizerD.step()\n",
    "          ############################\n",
    "          # (2) Update G network\n",
    "          ###########################\n",
    "          netG.zero_grad()\n",
    "          outputF = netD(fake)   \n",
    "          errG = (torch.mean((outputR - torch.mean(outputF) + labels) ** 2) +\n",
    "                  torch.mean((outputF - torch.mean(outputR) - labels) ** 2))/2\n",
    "          errG.backward()\n",
    "          optimizerG.step()\n",
    "          \n",
    "          if (ii+1) % (len(train_loader)//2) == 0:\n",
    "              print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f'\n",
    "                    % (epoch + 1, epochs, ii+1, len(train_loader),\n",
    "                      errD.item(), errG.item()))\n",
    "\n",
    "      show_generated_img()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t6CAY9SftOcF"
   },
   "outputs": [],
   "source": [
    "train(epochs = 30, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2bU08L-AQ3X1"
   },
   "outputs": [],
   "source": [
    "torch.save(netG.state_dict(), ROOT + \"generator26_3.h5\")\n",
    "torch.save(netD.state_dict(), ROOT + \"discriminator26_3.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ua8f1AXbTkqn",
    "outputId": "38d5c841-420b-4791-dc73-06cf5058ddbd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "netG.load_state_dict(torch.load(ROOT + \"generator26_3.h5\"))\n",
    "netD.load_state_dict(torch.load(ROOT + \"discriminator26_3.h5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VMUJQmjeLD7g"
   },
   "outputs": [],
   "source": [
    "gen_z = torch.randn(32, nz, 1, 1, device=device)\n",
    "gen_images = (netG(gen_z).to(\"cpu\").clone().detach() + 1)/2\n",
    "gen_images = gen_images.numpy().transpose(0, 2, 3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pBQuyGndLG32"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(25, 16))\n",
    "for ii, img in enumerate(gen_images):\n",
    "    ax = fig.add_subplot(4, 8, ii + 1, xticks=[], yticks=[])\n",
    "    plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u6yVUNVEOIp3"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('/content/drive/MyDrive/Colab Notebooks/out_put_image256'):\n",
    "    os.mkdir('/content/drive/MyDrive/Colab Notebooks/out_put_image256')\n",
    "im_batch_size = 50\n",
    "n_images=500\n",
    "for i_batch in range(0, n_images, im_batch_size):\n",
    "    gen_z = torch.randn(im_batch_size, nz, 1, 1, device=device)\n",
    "    gen_images = (netG(gen_z) + 1)/2\n",
    "    images = gen_images.to(\"cpu\").clone().detach()\n",
    "    images = images.numpy().transpose(0, 2, 3, 1)\n",
    "    for i_image in range(gen_images.size(0)):\n",
    "        save_image(gen_images[i_image, :, :, :], os.path.join('/content/drive/MyDrive/Colab Notebooks/out_put_image256', f'image_{i_batch+i_image:05d}.png'))\n",
    "\n",
    "# torch.save(gen_images.state_dict(), '/content/model.json')\n",
    "\n",
    "import shutil\n",
    "shutil.make_archive('images', 'zip', '/content/drive/MyDrive/Colab Notebooks/out_put_image256')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qg6-MtCICxml"
   },
   "outputs": [],
   "source": [
    "gen_z = torch.randn(32, nz, 1, 1, device=device)\n",
    "gen_images = (netG(gen_z).to(\"cpu\").clone().detach() + 1)/2\n",
    "gen_images = gen_images.numpy().transpose(0, 2, 3, 1)\n",
    "\n",
    "fig = plt.figure(figsize=(25, 16))\n",
    "for ii, img in enumerate(gen_images):\n",
    "    ax = fig.add_subplot(4, 8, ii + 1, xticks=[], yticks=[])\n",
    "    plt.imshow(img)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "RaLSGan.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
